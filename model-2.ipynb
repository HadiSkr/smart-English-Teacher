{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_dict = {\n",
    "    'er': 0,\n",
    "    'aa': 1,\n",
    "    'ahn': 2,\n",
    "    'ah': 3,\n",
    "    'tq': 4,\n",
    "    'nx': 5,\n",
    "    'jh': 6,\n",
    "    'own': 7,\n",
    "    'ihn': 8,\n",
    "    'hh': 9,\n",
    "    'ey': 10,\n",
    "    'v': 11,\n",
    "    'l': 12,\n",
    "    'dh': 13,\n",
    "    'uh': 14,\n",
    "    'f': 15,\n",
    "    'r': 16,\n",
    "    'uw': 17,\n",
    "    'aen': 18,\n",
    "    's': 19,\n",
    "    'ao': 20,\n",
    "    'b': 21,\n",
    "    't': 22,\n",
    "    'ih': 23,\n",
    "    'th': 24,\n",
    "    'oy': 25,\n",
    "    'SIL': 26,\n",
    "    'ch': 27,\n",
    "    'eng': 28,\n",
    "    'eh': 29,\n",
    "    'p': 30,\n",
    "    'sh': 31,\n",
    "    'ehn': 32,\n",
    "    'ay': 33,\n",
    "    'z': 34,\n",
    "    'w': 35,\n",
    "    'g': 36,\n",
    "    'm': 37,\n",
    "    'aw': 38,\n",
    "    'em': 39,\n",
    "    'el': 40,\n",
    "    'ae': 41,\n",
    "    'd': 42,\n",
    "    'k': 43,\n",
    "    'dx': 44,\n",
    "    'en': 45,\n",
    "    'y': 46,\n",
    "    'ow': 47,\n",
    "    'ng': 48,\n",
    "    'iy': 49,\n",
    "    'zh': 50,\n",
    "    'n': 51\n",
    "}\n",
    "\n",
    "excluded_segments = {\"aan\",\"iyn\",\"ayn\",\"eyn\",\"awn\",\"aon\",\"oyn\",\"uwn\",\"uhn\",\"a\",\"h\",\"e\",\"q\",\"ern\",\"uw ix\",\"i\",\"an\",\"hhn\",\"no\",\"j\",\"x\", \"id\", \"ah ix\", \"ih l\", \"ah n\", \"ah r\", \"ah l\", \"VOCNOISE\", \"NOISE\", \"EXCLUDE\", \"<EXCLUDE>\",\n",
    "                     \"IVER y\", \"IVER-LAUGH\", None, \", \", \"None\", \"{E_TRANS}\", \"<exclude-Name>\", \"LAUGH\", \"IVER\",\n",
    "                     \"{B_TRANS}\", \"UNKNOWN\", \"<EXCLUDE-name>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching the buckeye dataset .....\n",
      "sad2\n",
      "sad3\n",
      "Length of phoneme_intervals: 36843\n",
      "Length of phoneme_labels: 36843\n",
      "Length of frame_indices: 36843\n",
      "Length of aligned_frames: 36843\n",
      "Length of aligned_labels: 36843\n",
      "Shape of mfccs: (20, 21642)\n",
      "Length of truncated_aligned_frames: 36843\n",
      "Shape of aligned_frames: (36843, 20, 24)\n",
      "Shape of aligned_labels: (36843,)\n",
      "bye\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import buckeye\n",
    "import cleaneddata\n",
    "import numpy as np\n",
    "print(\"fetching the buckeye dataset .....\")\n",
    "\n",
    "phoneme_intervals = []\n",
    "phoneme_labels = []\n",
    "frame_duration = 0.01\n",
    "excluded_segments = cleaneddata.excluded_segments\n",
    "phoneme_dict = cleaneddata.phoneme_dict\n",
    "sp = 0\n",
    "corpus = buckeye.corpus('D:\\\\buckeye_dataset', load_wavs=True)\n",
    "\n",
    "for speaker in corpus:\n",
    "    if sp == 2:\n",
    "        break\n",
    "    for track in speaker:\n",
    "        audio_file = fr\"E:\\buckeye_wav\\{track.name}.wav\"\n",
    "        audio, sr = librosa.load(audio_file)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, hop_length=441)\n",
    "\n",
    "        for phone in track.phones:\n",
    "            if phone.dur > 0.01 and phone.seg not in excluded_segments:\n",
    "                start_time = phone.beg\n",
    "                end_time = phone.end\n",
    "                phoneme_intervals.append((start_time, end_time))\n",
    "                phoneme_labels.append(phoneme_dict[phone.seg])\n",
    "    sp += 1\n",
    "\n",
    "print(\"sad2\")\n",
    "\n",
    "frame_indices = []\n",
    "for interval in phoneme_intervals:\n",
    "    start_frame = int(interval[0] / frame_duration)\n",
    "    end_frame = int(interval[1] / frame_duration)\n",
    "    frame_indices.append((start_frame, end_frame))\n",
    "\n",
    "print(\"sad3\")\n",
    "\n",
    "aligned_frames = []\n",
    "aligned_labels = []\n",
    "for indices in frame_indices:\n",
    "    start_frame, end_frame = indices\n",
    "    frames = mfccs[:, start_frame:end_frame]\n",
    "    aligned_frames.append(frames)\n",
    "    aligned_labels.append(phoneme_labels[len(aligned_frames) - 1])\n",
    "\n",
    "# Print the length and shape of each list\n",
    "print(\"Length of phoneme_intervals:\", len(phoneme_intervals))\n",
    "print(\"Length of phoneme_labels:\", len(phoneme_labels))\n",
    "print(\"Length of frame_indices:\", len(frame_indices))\n",
    "print(\"Length of aligned_frames:\", len(aligned_frames))\n",
    "print(\"Length of aligned_labels:\", len(aligned_labels))\n",
    "\n",
    "print(\"Shape of mfccs:\", mfccs.shape)\n",
    "\n",
    "# Determine the maximum shape of the subarrays\n",
    "\n",
    "\n",
    "# Calculate the average length of the aligned frames\n",
    "\n",
    "scaling_factor = 6\n",
    "\n",
    "\n",
    "# Calculate the average length of the aligned frames\n",
    "average_length = int(np.mean([len(frames[0]) for frames in aligned_frames]) * scaling_factor)\n",
    "\n",
    "# Truncate longer sequences and pad shorter sequences\n",
    "truncated_aligned_frames = []\n",
    "for frames in aligned_frames:\n",
    "    if len(frames[0]) > average_length:\n",
    "        truncated_frames = frames[:, :average_length]\n",
    "    else:\n",
    "        padding = np.zeros((frames.shape[0], average_length - len(frames[0])))\n",
    "        truncated_frames = np.concatenate((frames, padding), axis=1)\n",
    "    truncated_aligned_frames.append(truncated_frames)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "aligned_frames = np.array(truncated_aligned_frames)\n",
    "\n",
    "\n",
    "# Print the length and shape of the modified array\n",
    "print(\"Length of truncated_aligned_frames:\", len(truncated_aligned_frames))\n",
    "print(\"Shape of aligned_frames:\", aligned_frames.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Print the length and shape of the modified array\n",
    "aligned_labels = np.array(aligned_labels)\n",
    "print(\"Shape of aligned_labels:\", aligned_labels.shape)\n",
    "\n",
    "\n",
    "print(\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Assuming alignes\n",
    "# d_frames is X and aligned_labels is y\n",
    "#print(tf.config.list_physical_devices('GPU'))\n",
    "X = aligned_frames  # Shape: (num_samples, timesteps, input_dim)\n",
    "Y = aligned_labels  # Shape: (num_samples,)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming aligned_frames is your input data X\n",
    "\n",
    "# Reshape the input data to 2D (flatten the frames)\n",
    "\n",
    "\n",
    "# Now X_normalized contains the normalized input data\n",
    "\n",
    "#num_classes = len(np.unique(y))  # Number of unique phoneme classes\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=12)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(20, 24), return_sequences=True))  # Input shape: (timesteps, input_dim)\n",
    "model.add(Dropout(0.2))  # Dropout layer with 10% dropout rate\n",
    "model.add(LSTM(128))  # Hidden LSTM layer\n",
    "model.add(Dropout(0.2))  # Dropout layer with 10% dropout rate\n",
    "model.add(Dense(52, activation='sigmoid'))  # Output shape: (num_classes,)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=50, batch_size=16, validation_split=0., callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
